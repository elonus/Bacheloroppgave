\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsthm} % theorems, definitions etc.

\usepackage{hyperref} % hyperlinks and pdf contents

\title{Bachelor}
\author{Andreas Matre}
\date{February 2020}

\begin{document}

\newtheorem{definition}{Definition}[subsection]
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}[section]


\maketitle

\tableofcontents

\section{Introduction}

When you want to model some relationship between a response and predictors you
first need to collect data to fit that model. This data can be collected in many
different ways and it is very important to take into account the methods used in
the information gathering when fitting models. This thesis will show how to fit
a linear regression model when data is collected through a complex survey, i.e.
a survey including unequal sampling probabilities, stratification and
clustering. Definitions and more details comes later in the thesis. We start
with an example illustrating what can go wrong if the methods used in the
information gathering is not taken into account.


\begin{example}

This example will show what can happen if sampling design is not taken
into account when doing regression. We will illustrate this using a dataset from a study which tried to show a relationship between the length of a persons left middle finger and their height. The dataset contains 200 samples, each containing the length of the persons left middle finger (cm), their height (inches) and the probability that they would be chosen for the sample.

The researcher wanted a bigger sample of shorter people than tall people so they sampled with probabilities proportional to: 24 for people with height < 65, 12 for people with height = 65, 2 for people with height = 66 or 67 and 1 for people with height > 67.


This means that a disproportionately big part of the sample contains short people. 

To illustrate the difference, this first plot shows a random sample where every person had an equal chance of being included. The next plot shows the sample where short people had a higher chance of being included than tall people. As you can see the second plot is much more concentrated in the bottom of the plot.

% Plot of apiSRS

% Plot of api unequal prob sample

This means that if you try to fit a linear regression model to the unequal probabilities sample the slope will tend to be smaller than what it should be, since there are few observations in the top right of the plot.

I will illustrate this now by fitting both a naive linear model and a model that takes into account the sampling probabilities.


% Code and plot of regression lines

We can see in the code that in the model which takes the unequal sampling probabilities into account we first define a design object which in addition to the covariates includes information about the probabilities of each observation being chosen. Here we use a value called the weight of the observation which is the inverse of the sampling probability. The weight can be interpreted as how many other possible observations this observation represent. A lower sampling probability means that there are probably fewer similar observations in the dataset, so each of these observations has to count more to account for this disrepancy.

In this plot the points representing the observations have a radius proportional to the inverse of the sampling probability, so as you can see the observations in the top have a much smaller chance of being chosen that the ones in the bottom part. The red line is the regression line fitted by normal linear regression while the blue line is fitted by a model which takes the design into account. 

As you can see, the blue line has a larger slope than the red line, indicating it gives the points in the top a bigger "weight" than the points in the bottom part to account for their lower sampling probabilities.

\end{example}

\subsection{Data we are going to use}

In this thesis I will mainly use one dataset to illustrate the concepts
discussed. This is a dataset on the performance of students in schools in
California. This dataset has data on all 6194 schools with more than 100
students in California, with data collected
including: API scores in 1999 and 2000, which level of school it is
(elementary, middle, high), name of school, location of school, percentage of
students tested at the school, API targets, economic factors for students at the
school, class sizes, information of education of parents and qualification of teachers.

API scores is a metric testing the academic performance of the students at the school.

We will use this data as the population we will sample from, and we will take
different kinds of samples to illustrate the concepts introduces in this thesis.

I will start by giving an introduction to survey statistics, where I will give the necessary background theory needed to be able to talk about linear regression in this context.

\section{Introduction survey statistics}

There are two big differenses between ``normal'' statistics and Survey statistics.

The first difference is that in Survey statistics you approach the ''randomness'' from a different angle.
Usually in statistics you let individual values be stochastic variables
with some distribution and one observation is a sample from that stochastic
variable.

In Survey statistics on the other hand you say that the individual values are
fixed, i.e. my height is not random, every time you measure it you will get the
exact same result, if you discount measurement errors. Instead what is random is
who we sample. This leads us to a couple definitions:

\begin{definition} \label{def:sampUnit}
  A \textbf{sampling unit} is one "element" we want to sample. In the case of
  our API example this will be one school.
\end{definition}

\begin{definition} \label{def:sampPop}
The \textbf{sampling population}, or universe, $U = \{1, 2, 3, ..., N\}$, is a
finite set containing all the sampling units you are interested in. In our case
it will be all the schools in California having 100 or more students.
\end{definition}

To represent the elements in $U$ by natural numbers we just let the elements
have a arbitrary ordering and denote them by their indices. We do this for
simplicity of notation as the order of the elements do not matter in the inference.

\begin{definition} \label{def:sampFrame}
The \textbf{sampling frame} is the list of all sampling units you are going to sample from. Ideally the sampling frame and the sampling population would be the same, but that is not always to case. In some cases for example, you may not have a list of all the people who are eligible to vote, you may perhaps only have a list of the people who voted in the last election. This can cause a bias in the results.
\end{definition}

In the API case, the sampling population and the sampling frame are equal, since
we have a table of all the data and we will just choose rows from that table for
our samples. But there are cases where this is not the case. An example of this
could be if we were doing a political survey to try to predict who will win the
next election.
In that case our sampling population, who we are interested in information
about, would be everyone who are going to vote in the next election. It is
is impossible to get a list of them though, so we instead have to use choose
some other part of the population we have information about. We might have a
list of all who voted last election, which might be a good approximation of
those who are going to vote this election, but then we would miss out on all the
new eligible voters and people who might have decided to vote this election but
didn't do it the last one.

Choosing the correct sampling frame to match your target sampling population is
difficult and getting it wrong can make your results wrong.


\begin{definition} \label{def:sample}
A \textbf{sample}, $S \subseteq U$, is a subset of the sampling frame. This is the data we will analyse to try to get information about the sampling population.
\end{definition}

We will denote the sampling units chosen in the sample by their indices in the
sampling population.

\begin{definition} \label{def:probSample}
A \textbf{probability sample} is a sample where the elements sampling units included are chosen randomly.
\end{definition}

\begin{definition} \label{def:sampProb}
The \textbf{sampling probability} of a sampling unit is the probability that a specific sampling unit is included in the sample.
\end{definition}

So we have a probability sample, where every sampling unit has some probability to be included in the sample, called the sampling probability.


The other big difference is that normally you assume that you sample from an
infinity population which has some distribution. In Survey statistics however we
have a finite distribution which means that it no longer makes sense thinking
about it having a distribution. This means that we no longer need the assumption
of which distribution the sampling units come from. This gives us more freedom,
and less potential problems if our assumptions are wrong.


The goal of Survey statistics is generally to estimate some statistic of the whole
population from a sample. These statistics include the total of some value,
f.eks. the total number of students in school, the mean of some value, f.eks.
the mean number of students per teacher in school, or to estimate regression
coefficients to either do inference or predict future values.

I will first start by talking about different kinds of samples along with some
inference on some statistics calculated from these samples and then finish with
talk about regression.

\section{Model based simple linear regression} \label{sec:modLinReg}

We will first give a short summary of normal simple linear regression, in a
model based context. We have a response variable, $y$, and a predictor, $x$. The
relationship between $y$ and $x$ is assumed to be:

\begin{equation*}
Y = \beta_0 + \beta_1 x + \epsilon,
\end{equation*}
where $\epsilon$ is an error term.

I.e. we assume there exists $\beta_0$ and $\beta_1$ that describes this
relationship perfectly.
We do not know these values however, so we instead have to estimate them given
independent observed data points. We call these data points $(x_1, y_1), (x_2, y_2)
... (x_n, y_n)$. To do
this we want to find $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize the
Residual Sum of Squared (RSS). The RSS is defined by:

\begin{align*}
  RSS &= \frac{1}{n} \sum_{i = 1}^N \left( y_i - \hat{y}_i \right)^2 \\
  &= \frac{1}{n} \sum_{i = 1}^N \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)^2
\end{align*}

The $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimizes the RSS are:
%$$
%\hat{\beta}_1 = \frac{\sum_{i = 1}^N\left( x_i - \bar{x} \right) \left( y_i -
    %\bar{y} \right)} {\sum_{i = 1}^N \left( x_i - \bar{x} \right)}
%$$
%and
%$$
%\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
%$$

\begin{equation*}
\hat{\beta}_1 = \frac{\sum_{i = 1}^n y_i x_i - \frac{1}{n} \left( \sum_{i = 1}^n
    y_i \right)  \left( \sum_{j = 1}^n x_j \right)}
  {\sum_{j = 1}^n x_j^2 - \frac{1}{n} \left( \sum_{j = 1}^n x_j \right)^2}
\end{equation*}

\begin{equation*}
\hat{\beta}_0 = \frac{1}{n} \left( \sum_{i = 1}^n y_i -
  \frac{\left( \sum_{i = 1}^n y_i x_i  \right) \left( \sum_{j = 1}^n
    x_j  \right) - \frac{1}{n} \left( \sum_{i = 1}^n y_i \ \right) \left( \sum_{j = 1}^n x_j \right)^2}
  {\sum_{j = 1}^n x_j^2 - \frac{1}{n} \left( \sum_{j = 1}^n x_j \right)^2}
\right)
\end{equation*}


Under some conditions it's shown that these estimates are the best estimates
you can get for $\beta_0$ and $\beta_1$. These conditions are:

\begin{enumerate}
\item $E \left( \epsilon_i \right) = 0\ \forall \ i = 1, 2, ..., n$
\item $Var \left( \epsilon_i \right) = \sigma^2\ \forall \ i = 1, 2, ..., n$
\item All the $\epsilon$ are independent of any predictor or observation number.
\item All $\epsilon_1$, $\epsilon_2$, ..., $\epsilon_n$ are independent of each other.
\item All $\epsilon$ are normally distributed.
\end{enumerate}

If all these assumption are followed get that the estimates are unbiased and:
\begin{equation*}
  Var \left( \hat{\beta}_0 \right) = \sigma^2 \frac{\sum_{i = 1}^N x_i^2}{n
    \sum_{i = 1}^N \left( x_i - \bar{x} \right)^2}
\end{equation*}
  

\begin{equation*}
  Var \left( \hat{\beta}_1 \right) = \frac{\sigma^2}{
    \sum_{i = 1}^N \left( x_i - \bar{x} \right)^2}
\end{equation*}

\section{Regression in the context of finite populations}

In the case of finite populations things change a bit. We still want to find the
coefficients that minimize the RSS, but since the population is finite our goal
is to find the line that minimizes the RSS for the whole population, i.e. find
$B_0$ and $B_1$ such that $\sum_{i = 1}^N (y_i - B_0 - B_1 x_i)$ is as small as
possible.

If we know the whole population we can just compute them using the same formulas
as in Section \ref{sec:modLinReg}, which if we rewrite them we get:

\begin{equation*}
B_1 = \frac{\sum_{i = 1}^N y_i x_i - \frac{1}{N} \left( \sum_{i = 1}^N
    y_i \right)  \left( \sum_{j = 1}^N x_j \right)}
  {\sum_{j = 1}^N x_j^2 - \frac{1}{N} \left( \sum_{j = 1}^N x_j \right)^2}
\end{equation*}

\begin{equation*}
B_0 = \frac{1}{N} \left( \sum_{i = 1}^N y_i -
  \frac{\left( \sum_{i = 1}^N y_i x_i  \right) \left( \sum_{j = 1}^N
      x_j  \right) - \frac{1}{N} \left( \sum_{i = 1}^N y_i \ \right) \left( \sum_{j = 1}^N x_j \right)^2}
  {\sum_{j = 1}^N x_j^2 - \frac{1}{N} \left( \sum_{j = 1}^N x_j \right)^2}
\right)
\end{equation*}

If we let $t_x = \sum_{i = 1}^N x_i$, $t_y = \sum_{i = 1}^N y_i, t_{x^2} =
\sum_{i = 1}^N x_i^2$ and $t_{xy} =
\sum_{i = 1}^N x_i y_i$, we instead get:

\begin{equation*}
  B_0 = \frac{1}{N} \left( t_y - \frac{t_{xy} t_x - \frac{1}{N} t_y t_x^2}
    {t_{x^2} - \frac{1}{N} t_x^2}
   \right)
\end{equation*}

\begin{equation*}
  B_1 = \frac{t_{xy} - \frac{1}{N} t_y t_x}
    {t_{x^2} - \frac{1}{N} t_x^2}
\end{equation*}

The case where we know the whole population however is not relevant for this
thesis, we are instead interested in the case where we have to sample from the
population to estimate such statistics.

If we then let $\hat{t}_x, \hat{t}_y, \hat{t}_{x^2}, \hat{t}_{xy}, \hat{N}$ be estimators
for $t_x, t_y, t_{x^2},
t_{xy}, N$ respectively we get the estimators for $B_0$ and $B_1$:

\begin{equation*}
  \hat{B}_0 = \frac{1}{\widehat{N}} \left( \hat{t}_y - \frac{\hat{t}_{xy} \hat{t}_x - \frac{1}{\widehat{N}} \hat{t}_y \hat{t}_x^2}
    {\hat{t}_{x^2} - \frac{1}{\widehat{N}} \hat{t}_x^2}
    \right)
\end{equation*}

\begin{equation*}
  \hat{B}_1 = \frac{\hat{t}_{xy} - \frac{1}{\widehat{N}} \hat{t}_y \hat{t}_x}
    {\hat{t}_{x^2} - \frac{1}{\widehat{N}} \hat{t}_x^2}
\end{equation*}

\subsection{Simple random sample}

One of the simpler probability samples is a Simple Random Sample (SRS). A
sample of size $M \leq N$ is an SRS if every subset $S \subseteq U$ has the same
probability of being chosen.

Lets say for example that $U = \{1, 2, 3, 4\}$ and that we want a sample, $S$ of
size $3$. Then there are $\binom{4}{3} = 4$  possible samples of size $3$:

\begin{equation*} S_1 = \{1, 2, 3\} \end{equation*}
\begin{equation*} S_2 = \{1, 2, 4\} \end{equation*}
\begin{equation*} S_3 = \{1, 3, 4\} \end{equation*}
\begin{equation*} S_4 = \{2, 3, 4\} \end{equation*}.

For this to be a SRS each of these subsets need to have the same probability of
being chosen, i.e. $P(S_1) = P(S_2) = P(S_3) = P(S_4) = 0.25$. A consequence of
having a SRS is that all the sampling probabilities are all equal, $P(1 \in S) =
P(2 \in S) = P(3 \in S) = P(4 \in S) = 0.75$. But having equal sampling
probabilities is not sufficient for the sample to be an SRS.

Look for example at this case:
Assume we want a sample of size $2$ from a population of size $4$, and that
$P(\{1, 3\}) = 0.5$ and $P(\{2, 4\}) = 0.5$ while the probabilities of all the
other possible samples are $0$. Then $P(1 \in S) = P(2 \in S) = P(3 \in S) = P(4 \in S) = 0.5$
but this is not a SRS since all possible subsets of size $2$ do not have equal
probability of being chosen.


\subsection{Estimation}

One of the more common statistics we want to estimate is the total of some value
for the whole population. We might for example want to estimate the total number
of students enrolled in school, in our API example.

We would then denote the value of interest in one school, i.e. the number of
enrolled students by $y_i$ and let
\begin{equation*}
  t_y = \sum_{i = 1}^{N} y_i
\end{equation*}
be the value we want to estimate.

The natural estimate of this total, if we sample $n$ elements, would be
\begin{equation*}
\hat{t}_y = \frac{N}{n}\sum_{i \in S} y_i
\end{equation*}
where we take the average of the values in our sample and then scale it up to
the whole population.

It can be shown using indicator variables and sampling probabilities that
$\hat{t}_y$ is an unbiased estimator for $t_y$.


A statistic related to the total is the mean of some value. We denote the true
mean in the population
\begin{equation*}
\bar{y}_U = \frac{1}{N} \sum_{i = 1}^{N} y_i
\end{equation*}
and the sample mean
\begin{equation*}
\bar{y}_S = \frac{1}{n} \sum_{i \in S} y_i ,
\end{equation*}
which we use as our estimator.

Then we can rewrite the estimate for $t$ as:
\begin{equation*}
\hat{t}_y = \frac{N}{n} \sum_{i \in S} y_i = N\bar{y}_S
\end{equation*}

Using the fact that $\hat{t}_y$ is unbiased it is trivial to show that
$\bar{y}_S$ is an unbiased estimator for $\bar{y}_U$.

It can be shown that the variance of the estimator is of the form:
\begin{equation*}
Var \left( \bar{y}_S \right) = \frac{S^2}{n} \left( 1 - \frac{n}{N}
\right)
\end{equation*}

where

\begin{equation*}
S^2 = \frac{1}{N - 1} \sum_{i = 1}^N (y_i - \bar{y}_U)^2
\end{equation*}

is the variance of the whole population.

We do not, however, know $S^2$ as that would require us to know to $y$ values
for the whole population $U$. Instead we estimate $S^2$ by:

\begin{align*}
  s^2
  &= \frac{1}{n - 1} \sum_{i \in S} \left( y_i - \bar{y}_S \right)^2
\end{align*}

which gives us the estimate of $Var(\bar{y}_U)$:
\begin{align*}
  \widehat{Var(\bar{y}_U)}
  &= \frac{1}{n} \left( 1 - \frac{n}{N} \right) s^2
\end{align*}

Since $s^2$ is unbiased we have that $\frac{s^2}{n} \left( 1 - \frac{n}{N}
\right)$ is an unbiased estimator of the variance of $\bar{y}_S$.

From this it follows that the variance of $\hat{t}_S$ has the unbiased estimator
$\frac{N^2}{n} s^2 \left( 1 - \frac{n}{N} \right)$.

The factor $\left( 1 - \frac{n}{N} \right)$ in the variances is what differs in
these variance estimates compared to the ``normal'' ones. It is called the
\textbf{finite population coefficient (fpc)} and comes from the fact that we are
sampling without replacement from a finite population.

When we sample a larger and larger portion on the population we will get more
and more information and therefore the variance will decrease towards zero. This
means that if we sample half the population the variance estimate will be half
of the variance estimate in model based statistics.


\subsection{Stratification}

\subsection{Clustering}

\subsection{Complex surveys}

\subsection{Variance estimation (?)}




\end{document}
