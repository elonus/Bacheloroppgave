\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}

\title{Bachelor}
\author{Andreas Matre}
\date{February 2020}

\begin{document}

\newtheorem{definition}{Definition}[subsection]
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}[section]


\maketitle

\tableofcontents

\section{Introduction}

When you want to make a model of some relationship between a response and predictors you generally first need to collect data to fit that model. This data can be collected in many different ways and it is very important to take into account the methods used in the information gathering when fitting models. In this paper I will show how to fit a linear regression model when your data is collected through a complex survey, a survey including unequal sampling probabilities, stratification and clustering. Definitions and details regarding what this means will come later in the paper. I will first start with a example illustrating what can go wrong if you do not take the methods used in the information gathering into account.


\begin{example}

I will here show a simple example which illustrates what can happen if we do not take into account the sampling design when doing regression. In this example we have a dataset from a study which tried to show a relationship between the length of a persons left middle finger and their height. The dataset contains 200 samples, each containing the length of the persons left middle finger (cm), their height (inches) and the probability that they would be chosen for the sample.

In this case the researcher wanted a bigger sample of shorter people than tall people so they sampled with probabilities proportional to: 24 for people with height < 65, 12 for people with height = 65, 2 for people with height = 66 or 67 and 1 for people with height > 67.


This means that a disproportionately big part of the sample contains short people. 

To illustrate the difference, this first plot shows a random sample where every person had an equal chance of being included. The next plot shows the sample where short people had a higher chance of being included than tall people. As you can see the second plot is much more concentrated in the bottom of the plot.

% Plot of apiSRS

% Plot of api unequal prob sample

This means that if you try to fit a linear regression model to the unequal probabilities sample the slope will tend to be smaller than what it should be, since there are few observations in the top right of the plot.

I will illustrate this now by fitting both a naive linear model and a model that takes into account the sampling probabilities.


% Code and plot of regression lines

We can see in the code that in the model which takes the unequal sampling probabilities into account we first define a design object which in addition to the covariates includes information about the probabilities of each observation being chosen. Here we use a value called the weight of the observation which is the inverse of the sampling probability. The weight can be interpreted as how many other possible observations this observation represent. A lower sampling probability means that there are probably fewer similar observations in the dataset, so each of these observations has to count more to account for this disrepancy.

In this plot the points representing the observations have a radius proportional to the inverse of the sampling probability, so as you can see the observations in the top have a much smaller chance of being chosen that the ones in the bottom part. The red line is the regression line fitted by normal linear regression while the blue line is fitted by a model which takes the design into account. 

As you can see, the blue line has a larger slope than the red line, indicating it gives the points in the top a bigger "weight" than the points in the bottom part to account for their lower sampling probabilities.

\end{example}

\subsection{Data we are going to use}

In this thesis I will mainly use one dataset to illustrate the concepts
discussed. This is a dataset on the performance of students in schools in
California. This dataset has data on all 6194 schools with more than 100
students in California, with data collected
including: API scores in 1999 and 2000, which level of school it is
(elementary, middle, high), name of school, location of school, percentage of
students tested at the school, API targets, economic factors for students at the
school, class sizes, information of education of parents and qualification of teachers.

API scores is a metric testing the academic performance of the students at the school.

We will use this data as the population we will sample from, and we will take
different kinds of samples to illustrate the concepts introduces in this thesis.

I will start by giving an introduction to survey statistics, where I will give the necessary background theory needed to be able to talk about linear regression in this context.

\section{Introduction survey statistics}

There are two big differenses between ``normal'' statistics and Survey statistics.

The first difference is that in Survey statistics you approach the ''randomness'' from a different angle.
Usually in statistics you let individual values be stochastic variables
with some distribution and one observation is a sample from that stochastic
variable.

In Survey statistics on the other hand you say that the individual values are
fixed, i.e. my height is not random, every time you measure it you will get the
exact same result, if you discount measurement errors. Instead what is random is
who we sample. This leads us to a couple definitions:

\begin{definition} \label{def:sampUnit}
  A \textbf{sampling unit} is one "element" we want to sample. In the case of
  our API example this will be one school.
\end{definition}

\begin{definition} \label{def:sampPop}
The \textbf{sampling population}, or universe, $U = \{1, 2, 3, ..., N\}$, is a
finite set containing all the sampling units you are interested in. In our case
it will be all the schools in California having 100 or more students.
\end{definition}

To represent the elements in $U$ by natural numbers we just let the elements
have a arbitrary ordering and denote them by their indices. We do this for
simplicity of notation as the order of the elements do not matter in the inference.

\begin{definition} \label{def:sampFrame}
The \textbf{sampling frame} is the list of all sampling units you are going to sample from. Ideally the sampling frame and the sampling population would be the same, but that is not always to case. In some cases for example, you may not have a list of all the people who are eligible to vote, you may perhaps only have a list of the people who voted in the last election. This can cause a bias in the results.
\end{definition}

In the API case, the sampling population and the sampling frame are equal, since
we have a table of all the data and we will just choose rows from that table for
our samples. But there are cases where this is not the case. An example of this
could be if we were doing a political survey to try to predict who will win the
next election.
In that case our sampling population, who we are interested in information
about, would be everyone who are going to vote in the next election. It is
is impossible to get a list of them though, so we instead have to use choose
some other part of the population we have information about. We might have a
list of all who voted last election, which might be a good approximation of
those who are going to vote this election, but then we would miss out on all the
new eligible voters and people who might have decided to vote this election but
didn't do it the last one.

Choosing the correct sampling frame to match your target sampling population is
difficult and getting it wrong can make your results wrong.


\begin{definition} \label{def:sample}
A \textbf{sample}, $S \subseteq U$, is a subset of the sampling frame. This is the data we will analyse to try to get information about the sampling population.
\end{definition}

We will denote the sampling units chosen in the sample by their indices in the
sampling population.

\begin{definition} \label{def:probSample}
A \textbf{probability sample} is a sample where the elements sampling units included are chosen randomly.
\end{definition}

\begin{definition} \label{def:sampProb}
The \textbf{sampling probability} of a sampling unit is the probability that a specific sampling unit is included in the sample.
\end{definition}

So we have a probability sample, where every sampling unit has some probability to be included in the sample, called the sampling probability.


The other big difference is that normally you assume that you sample from an
infinity population which has some distribution. In Survey statistics however we
have a finite distribution which means that it no longer makes sense thinking
about it having a distribution. This means that we no longer need the assumption
of which distribution the sampling units come from. This gives us more freedom,
and less potential problems if our assumptions are wrong.


The goal of Survey statistics is generally to estimate some statistic of the whole
population from a sample. These statistics include the total of some value,
f.eks. the total number of students in school, the mean of some value, f.eks.
the mean number of students per teacher in school, or to estimate regression
coefficients to either do inference or predict future values.

I will first start by talking about different kinds of samples along with some
inference on some statistics calculated from these samples and then finish with
talk about regression.

\section{Simple random sample}

One of the simpler probability samples is a Simple Random Sample (SRS). A
sample of size $M \leq N$ is an SRS if every subset $S \subseteq U$ has the same
probability of being chosen.

Lets say for example that $U = \{1, 2, 3, 4\}$ and that we want a sample, $S$ of
size $3$. Then there are $\binom{4}{3} = 4$  possible samples of size $3$:

$$ S_1 = \{1, 2, 3\} $$
$$ S_2 = \{1, 2, 4\} $$
$$ S_3 = \{1, 3, 4\} $$
$$ S_4 = \{2, 3, 4\} $$.

For this to be a SRS each of these subsets need to have the same probability of
being chosen, i.e. $P(S_1) = P(S_2) = P(S_3) = P(S_4) = 0.25$. A consequence of
having a SRS is that all the sampling probabilities are all equal, $P(1 \in S) =
P(2 \in S) = P(3 \in S) = P(4 \in S) = 0.75$. But having equal sampling
probabilities is not sufficient for the sample to be an SRS.

Look for example at this case:
Assume we want a sample of size $2$ from a population of size $4$, and that
$P(\{1, 3\}) = 0.5$ and $P(\{2, 4\}) = 0.5$ while the probabilities of all the
other possible samples are $0$. Then $P(1 \in S) = P(2 \in S) = P(3 \in S) = P(4 \in S) = 0.5$
but this is not a SRS since all possible subsets of size $2$ do not have equal
probability of being chosen.


\subsection{Estimation}

One of the more common statistics we want to estimate is the total of some value
for the whole population. We might for example want to estimate the total number
of students enrolled in school, in our API example.

We would then denote the value of interest in one school, i.e. the number of
enrolled students by $y_i$ and let
$$t = \sum_{i = 1}^{N}$$
be the value we want to estimate.

The natural estimate of this total, if we sample $n$ elements, would be
$$ \hat{t}_S = \frac{N}{n}\sum_{y \in S} y $$
where we take the average of the values in our sample and then scale it up to
the whole population.

To show that $\hat{t}_S$ is unbiased we rewrite the expression using the
indicator function:
$$ I(y \in S) =
\begin{cases}
  1 & , y \in S \\
  0 & , y \notin S
\end{cases} .$$

Then $$ \hat{t}_S = \frac{N}{n} \sum_{y \in S} y = \frac{N}{n} \sum_{i = 1}^{N}
I(y_i \in S) y_i ,$$
which means that
$$ E \left[ \hat{t}_S \right] = E \left[ \frac{N}{n} \sum_{i = 1}^{N}
  I(y_i \in S) y_i \right] =
\frac{N}{n} \sum_{i = 1}^{N}
E\left[ I(y_i \in S) \right] y_i .$$

We now only have to find the expected value of $I(y_i \in S)$.

\begin{align*}
  E \left[ I(y_i \in S) \right]
  &= 1 \cdot P(y_i \in S) + 0 \cdot P(y_i \in S)
    = \frac{\binom{N - 1}{n - 1}}{\binom{N}{n}} \\
  &= \frac{(N - 1)! n! (N - n)!}{N! (n - 1)! (N - n)!}
    = \frac{n}{N}
\end{align*}

which comes from the fact that this is a SRS and therefore all possible samples
are equally likely.

This finally gives us:
$$
E \left[ \hat{t}_S \right]
= \frac{N}{n} \sum_{i = 1}^{N} E\left[ I(y_i \in S) \right] y_i
= \frac{N}{n} \sum_{i = 1}^{N} \frac{n}{N} y_i
= \sum_{i = 1}^{N} y_i
= t
$$
which shows that $\hat{t}_S$ is an unbiased estimator of $t$.


A statistic related to the total is the mean of some value. We denote the true
mean in the population
$$\bar{y}_U = \frac{1}{N} \sum_{i = 1}^{N} y_i$$
and the sample mean
$$\bar{y}_S = \frac{1}{n} \sum_{y \in S} y ,$$
which we use as our estimator.

Then we can rewrite the estimate for $t$ as:
$$ \hat{t}_S = \frac{N}{n} \sum_{y \in S} y = N\bar{y}_S $$

\begin{align*}
E \left[ \bar{y}_S \right]
&= E \left[ \frac{1}{n} \sum_{y \in S} y \right]
= E \left[ \frac{1}{n} \sum_{i = 1}^N I(y_i \in S) y_i \right] \\
&= \frac{1}{n} \sum_{i = 1}^N E \left[ I(y_i \in S) \right] y_i
= \frac{1}{n} \sum_{i = 1}^N \frac{n}{N} y_i \\
&= \frac{1}{N} \sum_{i = 1}^N y_i
= \bar{y}_U
\end{align*}

So $\bar{y}_S$ is an unbiased estimator for $\bar{y}_U$.

To calculate the variance we need a couple of results first:
\begin{align*}
  Var \left[ I(y_i \in S) \right]
  &= E \left[ I(y_i \in S)^2 \right] - E \left[
  I(y_i \in S) \right]^2 \\
  &= P(y_i \in S) - P(y_i \in S)^2
  = \frac{n}{N}(1 - \frac{n}{N})
\end{align*}

\begin{align*}
  E\left[ I(y_i \in S)I(y_j \in S) \right]
  &= P(y_i \in S, y_j \in S) \\
  &= P(y_i \in S) P(y_j \in S | y_i \in S) \\
  &= \frac{n}{N} \frac{n - 1}{N - 1}
    ,
\end{align*}
where $i \neq j$.

Now we can find $Var(\bar{y}_S)$:
\begin{align*}
 Var \left[ \bar{y}_S \right] 
  &= Var \left[ \frac{1}{n} \sum_{i = 1}^N I(y_i \in S) y_i \right] \\
  &= \frac{1}{n^2} \left( \sum_{i = 1}^N  Var \left[ I(y_i \in S) y_i \right] + \sum_{i = 1}^{N}\sum_{j \neq i}^{N} Cov \left[ I(y_i \in S) y_i, I(y_j \in S) y_j \right] \right) \\
  &= \frac{1}{n^2} \left( \sum_{i = 1}^N y_i^2 Var \left[ I(y_i \in S) y_i \right] + \sum_{i = 1}^{N}\sum_{j \neq i}^{N} y_i y_j Cov \left[ I(y_i \in S), I(y_j \in S) \right] \right) \\
  &= \frac{1}{n^2} \left( \sum_{i = 1}^N y_i^2 Var \left[ I(y_i \in S) y_i \right] \\
  &\ \ \ \ + \sum_{i = 1}^{N}\sum_{j \neq i}^{N} y_i y_j \left( E \left[ I(y_i \in S) I(y_j \in S) \right] - E \left[ I(y_i \in S) \right] E \left[ I(y_j \in S) \right] \right) \right) \\
  &= \frac{1}{n^2} \left( \sum_{i = 1}^N y_i^2 \frac{n}{N}(1 - \frac{n}{N}) + \sum_{i = 1}^{N}\sum_{j \neq i}^{N} y_i y_j \left( \frac{n}{N} \frac{n - 1}{N - 1} - \frac{n^2}{N^2} \right) \right) \\
\end{align*}

To do the next step we need to do a couple of small calculations:

\begin{align*}
  \left( \frac{n}{N} \frac{n - 1}{N - 1} - \frac{n^2}{N^2} \right) 
  &= \frac{n}{N} \frac{N(n - 1) - n(N - 1)}{N(N - 1)} \\
  &= \frac{n}{N} \frac{n - N}{N(N - 1)} \\
  &= - \frac{n}{N} \frac{1}{N - 1} \left( 1 - \frac{n}{N} \right)
\end{align*}

Additionally:
\begin{align*}
  \sum_{i = 1}^{N}\sum_{j \neq i}^{N} y_i y_j
  &= \sum_{i = 1}^{N}\sum_{j = 1}^{N} y_i y_j - \sum_{i = 1}^{N} y_i^2
\end{align*}

which gives us:
\begin{align*}
  Var(\bar{y}_S)
  &= \frac{1}{n^2} \left( \sum_{i = 1}^N y_i^2 \frac{n}{N}(1 - \frac{n}{N}) + \sum_{i = 1}^{N}\sum_{j \neq i}^{N} y_i y_j \left( \frac{n}{N} \frac{n - 1}{N - 1} - \frac{n^2}{N^2} \right) \right) \\
  &= \frac{1}{n^2} \frac{n}{N} \left( 1 - \frac{n}{N} \right) \left( \sum_{i = 1}^{N} y_i^2 - \frac{1}{N - 1} \sum_{i = 1}^{N}\sum_{j = 1}^{N} y_i y_j + \frac{1}{N- 1} \sum_{i = 1}^{N} y_i^2 \right) \\
  &= \frac{1}{nN} \left( 1 - \frac{n}{N} \right) \left( \frac{N}{N - 1} \sum_{i = 1}^{N} y_i^2 - \frac{N^2}{N - 1} \bar{y}_U^2  \right)
  = \frac{N}{nN(N - 1)} \left( 1 - \frac{n}{N} \right) \left( \sum_{i = 1}^{N} y_i^2 -  N \bar{y}_U^2  \right) \\
  &= \frac{1}{n(N - 1)} \left( 1 - \frac{n}{N} \right) \sum_{i = 1}^{N} \left( y_i - \bar{y}_U \right)^2 
  = \frac{1}{n} \left( 1 - \frac{n}{N} \right) S^2 \\
\end{align*}

where

$$
S^2 = \frac{1}{N - 1} \sum_{i = 1}^N (y_i - \bar{y}_U)^2
$$

is the variance of the whole population.

We do not, however, know $S^2$ as that would require us to know to $y$ values
for the whole population $U$. Instead we will have to estimate the variance by:

\begin{align*}
  s^2
  &= \frac{1}{n - 1} \sum_{y \in S} \left( y - \bar{y}_S \right)^2
\end{align*}

which gives us the estimate of $Var(\bar{y}_U)$:
\begin{align*}
  \widehat{Var(\bar{y}_U)}
  &= \frac{1}{n} \left( 1 - \frac{n}{N} \right) s^2
\end{align*}

Now, if we can show that $s^2$ is an unbiased estimator for $S^2$ we have an
unbiased estimator for $\widehat{Var(\bar{y}_U)}$.

\begin{align*}
  E \left[ \sum_{y \in S} \left( y - \bar{y}_S \right) \right]
  &= E \left[ \sum_{y \in S} \left( y^2 - 2y\bar{y}_S + \bar{y}_S^2 \right) \right] \\
  &= E \left[ \sum_{i = 1}^{N} I(y_i \in S) y_i^2 - n \bar{y}_S^2 \right] \\
  &= \sum_{i = 1}^{N} \frac{n}{N} y_i^2 - n E \left[ \bar{y}_S \right]^2 - n Var \left[ \bar{y}_S \right] \\
  &= \frac{n}{N} \left( \sum_{i = 1}^{N} y_i^2 - \sum_{i = 1}^{N} 2y_i \bar{y}_U + \sum_{i = 1}^{N} \bar{y}_U^2  \right)- n \frac{1}{n} \left( 1 - \frac{n}{N} \right) S^2 \\
  &= \frac{n}{N} \sum_{i = 1}^{N} \left( y_i - \bar{y}_U \right)^2 - \left( 1 - \frac{n}{N} \right) S^2 \\
  &= \frac{n}{N} \left( N - 1 \right) S^2 - \left( 1 - \frac{n}{N} \right) S^2 \\
  &= \left( n - 1 \right) S^2
\end{align*}

This means that
\begin{align*}
 E \left[ s^2 \right] 
  &= S^2
\end{align*}

Which then gives us:
\begin{align*}
  E \left( \widehat{Var \left( \bar{y}_S \right)} \right)
  &= E \left( \frac{1}{n} \left( 1 - \frac{n}{N} \right) s^2 \right) \\
  &= Var \left( \bar{y}_S \right)
\end{align*}

\section{Stratification}

\section{Clustering}

\section{Complex surveys}

\section{Variance estimation (?)}

\section{Regression}






\end{document}
